{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Step 1 - Exploring the dataset"
      ],
      "metadata": {
        "id": "Vpi_ZwzKnqOY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zMofdWm4fExF",
        "outputId": "cee6004b-f810-45d1-a9c4-4475059c807c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.17.0\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "print(tf.version.VERSION)\n",
        "data = pd.read_csv(\"./IMDB Dataset.csv\")\n",
        "data['sentiment'] = data['sentiment'].map({'positive': 1, 'negative': 0})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z5lGfkEhfIlE",
        "outputId": "9e701f3f-d273-4cf2-c67a-c4cb9a3e8d03"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "positive :  25000  negative :  25000\n"
          ]
        }
      ],
      "source": [
        "positive = 0\n",
        "for k in range(len(data)):\n",
        "    if data[\"sentiment\"][k] == 1:\n",
        "        positive+=1\n",
        "\n",
        "print(\"positive : \",positive,\" negative : \", 50000-positive)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 2 - Tokenization and padding"
      ],
      "metadata": {
        "id": "tzFsyAc3nzM2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WLkUX1nhhScV",
        "outputId": "523a9c8f-dd68-4a37-92e3-4829d854b6ad"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[  28,    5,    2, ...,    0,    0,    0],\n",
              "       [   4,  394,  121, ...,    0,    0,    0],\n",
              "       [  11,  191,   12, ...,    0,    0,    0],\n",
              "       ...,\n",
              "       [  11,  236,    4, ...,    0,    0,    0],\n",
              "       [ 146,  167,    6, ...,    0,    0,    0],\n",
              "       [  55,   28, 5893, ...,    0,    0,    0]], dtype=int32)"
            ]
          },
          "metadata": {},
          "execution_count": 71
        }
      ],
      "source": [
        "# data = data[:100]\n",
        "\n",
        "# Tokenization using TensorFlow\n",
        "tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=10000, oov_token=\"<OOV>\")\n",
        "tokenizer.fit_on_texts(data['review'])\n",
        "sequences = tokenizer.texts_to_sequences(data['review'])\n",
        "\n",
        "#Padding\n",
        "\n",
        "padded_sequences = tf.keras.preprocessing.sequence.pad_sequences(sequences, padding='post')\n",
        "padded_sequences\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 3 - Evaluation Process of the Model\n",
        "\n",
        "Compare the returned results with the expected results, calculate loss functions, accuracy, precision, recall, etc, and return the most common words when it fails.\n",
        "\n",
        "EXPLAIN WHY WE USE A PARTICULAR METHOD (why it suits our model, etc)\n",
        "\n"
      ],
      "metadata": {
        "id": "Z25aclwql1Yl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 4 - Neural Network"
      ],
      "metadata": {
        "id": "NhxsA5Iu2hK7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x_train, x_test, y_train, y_test = train_test_split(padded_sequences, data[\"sentiment\"], test_size=0.2, random_state=42)\n",
        "\n",
        "print(len(x_test),len(y_test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zi0iYZvRn3xu",
        "outputId": "d492b241-e84e-4320-c4a1-49c389bc38f0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10000 10000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = models.Sequential([\n",
        "    layers.Embedding(input_dim=10000, output_dim=64),  # Example input_dim & output_dim\n",
        "    layers.GlobalAveragePooling1D(),\n",
        "    layers.Dense(4092, activation='relu'),\n",
        "    layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam',\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy','precision','recall'])\n",
        "\n",
        "model.fit(x_train, y_train, epochs=5, validation_data=(x_test, y_test))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lczwwDdLqOUS",
        "outputId": "3d1ebc3c-e278-4739-e4ed-e97ee6f92d80"
      },
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 5ms/step - accuracy: 0.5032 - loss: 0.6941 - precision: 0.4965 - recall: 0.3795 - val_accuracy: 0.5039 - val_loss: 0.6931 - val_precision: 0.5039 - val_recall: 1.0000\n",
            "Epoch 2/5\n",
            "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 4ms/step - accuracy: 0.5053 - loss: 0.6934 - precision: 0.5022 - recall: 0.4060 - val_accuracy: 0.4961 - val_loss: 0.6931 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 3/5\n",
            "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.5069 - loss: 0.6933 - precision: 0.3500 - recall: 0.1872 - val_accuracy: 0.5039 - val_loss: 0.6931 - val_precision: 0.5039 - val_recall: 1.0000\n",
            "Epoch 4/5\n",
            "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.5023 - loss: 0.6935 - precision: 0.5014 - recall: 0.6149 - val_accuracy: 0.5039 - val_loss: 0.6932 - val_precision: 0.5039 - val_recall: 1.0000\n",
            "Epoch 5/5\n",
            "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - accuracy: 0.5097 - loss: 0.6934 - precision: 0.5088 - recall: 0.4774 - val_accuracy: 0.4962 - val_loss: 0.6932 - val_precision: 1.0000 - val_recall: 1.9845e-04\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7e93341b3880>"
            ]
          },
          "metadata": {},
          "execution_count": 90
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exercice 2"
      ],
      "metadata": {
        "id": "sfiMV84M5vvf"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}